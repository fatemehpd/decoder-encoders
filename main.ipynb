{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hxj-Sj8-rRYn"},"outputs":[],"source":["import os\n","import sys\n","import glob\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torchvision.transforms as TF\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["sys.path.append(os.path.join(os. getcwd(),\"src\"))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[WinError 2] The system cannot find the file specified: '/src'\n","d:\\desk top folders\\ML\n","d:\\desk top folders\n"]}],"source":["\n","%cd /src\n","from dataset import CTDataset\n","from model import *\n","import utils \n","%cd ..\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train = CTDataset(\"./converted_dataset/train_img\",\"./converted_dataset/train_label\")\n","test = CTDataset(\"./converted_dataset/val_img\",\"./converted_dataset/val_label\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["config = pd.read_json('/content/drive/MyDrive/Colab Notebooks/APAC/config.json')\n","resnet152 = Resnet(\"resnet152\")\n","config = config[\"resnet152\"]\n","print(config)"]},{"cell_type":"markdown","metadata":{},"source":["# init"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","train_loader, val_loader = utils.get_loaders(\n","        os.path.join(config[\"train_path\"],\"image\"),\n","        os.path.join(config[\"train_path\"],\"mask\"),\n","        os.path.join(config[\"validation_path\"],\"image\"),\n","        os.path.join(config[\"validation_path\"],\"mask\"),\n","        config[\"batch_size\"],\n","        config[\"num_workers\"],\n","        config[\"pin_memory\"],\n","    )\n","\n","if config[\"load_model\"]:\n","        epoch_losses = np.load('losses\\\\epoch_losses.npy').tolist()\n","        val_losses = np.load('.\\\\losses\\\\val_losses.npy').tolist()\n","    else:\n","        epoch_losses = []\n","        val_losses = []\n","losses = []\n","\n","if config[\"optimizer\"] == \"Adam\":\n","    optimizer = optim.Adam(resnet152.getmodel().parameters(), lr=config(\"base_lr\"))    \n","    \n","loss_fn = []\n","if config[\"loss_function\"] == \"BCEWithLogitsLoss\":\n","    loss_fn.append(nn.BCEWithLogitsLoss())\n","if config[\"loss_function\"] == \"IoU\":\n","    loss_fn.append(IoULoss())\n","if config[\"loss_function\"] == \"dice\":\n","    loss_fn.append(DiceLoss())\n","if config[\"loss_function\"] == \"MSE\":\n","    loss_fn.append(nn.MSELoss())\n","#NOTE: temporary loss\n","loss_fn = nn.MSELoss()\n","if LOAD_MODEL:\n","        load_checkpoint(torch.load(config[\"check_point_path\"]), resnet152.get_model())\n","\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"markdown","metadata":{},"source":["# training function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_fn(loader, model, optimizer, loss_fn, scaler, losses):\n","    loop = tqdm(loader)\n","    for batch_idx, (data, targets) in enumerate(loop):\n","\n","        data = data\n","        targets = targets\n","\n","        # forward\n","        with torch.cuda.amp.autocast():\n","            pred = model(data)\n","            loss, _ = loss_fn(pred, targets)\n","            \n","        # backward\n","        optimizer.zero_grad()\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # update tqdm loop\n","        loop.set_postfix(loss=loss.item())\n","        losses.append(loss.item())"]},{"cell_type":"markdown","metadata":{},"source":["# training loop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(config[\"epochs\"]):\n","        print(f\"Number of Epoch: {epoch}\")\n","        # print(losses)\n","        if losses != []:\n","            epoch_losses.append(np.average(losses))\n","            losses = []\n","            np.save('losses\\\\epoch_losses', epoch_losses)\n","            print(epoch_losses[-1])\n","\n","        #TODO: manage loss_combined\n","        train_fn(train_loader, resnet152.get_model(), optimizer, loss_combined, scaler, losses)\n","\n","        # save model\n","        if(epoch % config[\"save_step\"] == 0):\n","            checkpoint = {\n","                \"state_dict\": resnet152.get_model().state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","            }\n","            save_checkpoint(checkpoint)\n","\n","            # print some examples to a folder\n","            save_predictions_as_imgs(\n","                val_loader, resnet152.get_model(), loss_combined, val_losses, folder=\"./saved_images\", device=DEVICE \n","            )"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOsyRMmIxTmeFtcD+8uRSZx","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
